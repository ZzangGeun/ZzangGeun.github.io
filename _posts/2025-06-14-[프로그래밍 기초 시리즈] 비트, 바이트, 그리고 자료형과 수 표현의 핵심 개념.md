---
layout: post
title: "비트, 바이트, 그리고 자료형과 수 표현의 핵심 개념"
date: 2025-06-14 10:00:00 +0900
categories: 프로그래밍 기초 시리즈
---

# 컴퓨터는 숫자를 어떻게 저장할까?

## 비트와 바이트의 개념

컴퓨터는 문자나 숫자의 의미를 이해하지 못합니다. 오직 **0과 1**, 두 가지 정보만을 사용해서 모든 데이터를 처리합니다. 그렇다면 어떻게 이진 정보만으로 우리가 사용하는 숫자와 문자를 표현할 수 있을까요?

### 비트(Bit)란?

**비트(bit)**는 **0과 1로 표현되는 정보의 최소 단위**입니다. 앞에서 컴퓨터는 숫자를 이해 못한다고 했는데 0과 1이 나오는 이유는 다음과 같이 이해하면 됩니다.  
- `0`: 전기가 흐르지 않음 (스위치 OFF)  
- `1`: 전기가 흐름 (스위치 ON)

하나의 비트로는 두 가지 정보(0, 1)를 표현할 수 있습니다. 여러 개의 비트가 모이면 더 많은 정보를 표현할 수 있게 됩니다.

| 비트 수 | 표현 정보 예시       | 정보의 개수 |
|---------|----------------------|-------------|
| 1       | 0, 1                 | 2           |
| 2       | 00, 01, 10, 11       | 4           |
| 3       | 000 ~ 111            | 8           |
| 4       | 0000 ~ 1111          | 16          |
| …       | …                    | …           |
| 8       | 00000000 ~ 11111111  | 256         |

8비트만 되어도 **256가지 정보**를 표현할 수 있습니다. 이처럼 0과 1로 수를 표현하는 방식을 **이진법**이라고 하며, 컴퓨터를 이해하기 위한 가장 중요한 개념입니다.

---

## 이진수는 어떻게 십진수로 바뀔까?

이진수를 십진수로 바꾸는 방법은 다음과 같습니다:

> **각 자리의 값 × (2의 제곱 수)** 를 모두 더하면 됩니다.

### 예시:
**1011** = (1×2³) + (0×2²) + (1×2¹) + (1×2⁰)  
→ 8 + 0 + 2 + 1 = **11**

---

## 바이트(Byte)란?

비트는 최소 단위이지만, 컴퓨터에서는 주로 **8비트 단위(1바이트)**로 데이터를 처리합니다.

> 💡 **1바이트 = 8비트**

그런데 왜 8비트를 1바이트라고 정한 걸까요? 왜 10비트가 아닌 8비트가 기준이 되었을까요?

---

## 바이트가 탄생한 이유

### 1. 초기 컴퓨터와 문자 인코딩

- **1950년대**: 알파벳 대문자, 숫자, 특수문자를 인코딩하기 위해 **6비트 문자 코드**가 사용됨 (64가지 정보).
- **1960년대**: 미국 표준협회(ANSI)가 만든 **7비트 ASCII**가 등장하여 128개의 문자(대소문자, 숫자, 특수문자, 제어문자 등)를 표현.

### 2. 8비트의 필요성과 확장성

7비트는 영어권에는 충분했지만, 다음과 같은 이유로 **8비트**가 사용되기 시작했습니다:

- ✅ **패리티 비트(오류 검출)**: 7비트 + 1비트로 오류 감지 가능
- ✅ **다국어 문자 지원**: 유럽어의 악센트, 특수 문자 등 표현 가능
- ✅ **그래픽 심볼 표현**: 문자 외 추가 기호도 표현 가능
- ✅ **하드웨어 설계의 효율성**: 8비트(2³)는 전자 회로 설계에 이상적인 단위였고, 처리 효율이 높음

이러한 이유로 대부분의 시스템이 8비트를 기본 단위로 사용하는 **바이트 중심 구조**로 발전하게 된 것입니다.

### 3. '바이트(Byte)'라는 용어의 정착
- ✅ **워드(Word)와의 혼동**: 초기에는 '워드(Word)'라는 용어를 썼지만, 워드는 컴퓨터 아키텍처마다 그 크기가 달랐습니다 (16비트 워드, 32비트 워드 등).
- ✅ **새로운 단위의 필요성**: 1960년대 후반, IBM 시스템/360 아키텍처가 8비트를 기본 데이터 처리 단위로 채택하면서, 이 8비트 단위를 '바이트(Byte)'라고 명명하고 텍스트 문자의 저장 단위로 명확히 정의했습니다. IBM 시스템/360은 당시 컴퓨터 산업에 지대한 영향을 미쳤고, 8비트 바이트는 사실상의 표준으로 자리 잡았습니다.

---

## 결론

정리하면,

- **비트**는 0과 1의 최소 정보 단위
- **바이트**는 8비트 묶음으로 정보를 저장하고 처리
- 이진법은 컴퓨터 내부 데이터 표현의 핵심 원리
- 8비트는 효율성과 확장성 면에서 이상적인 단위

앞으로 컴퓨터의 저장 방식, 데이터 타입(int, float 등), 메모리 구조 등을 이해할 때 이 기본 개념들이 중요한 기반이 됩니다.


---

## 데이터 타입과 표현 범위

### ✅ C/C++ (일부)

| 종류 | 타입       | 비트 수 | 표현 범위                 |
|------|------------|---------|----------------------------|
| 문자 | `char`     | 8       | -128 ~ 127                |
| 정수 | `short`    | 16      | -32,768 ~ 32,767          |
| 정수 | `int`      | 32      | -2,147,483,648 ~ 2,147,483,647 |
| 정수 | `long long` | 64      | $-2^{63}$ ~ $2^{63}-1$                  |
| 실수 | `float`    | 32      | $-3.4 × 10^{38}$ ~ $3.4 × 10^{38}$                  |
| 실수 | `double`   | 64      | $-1.7 × 10^{308}$~$1.7 × 10^{308}$                |




### ✅ 파이썬
| 종류  | 타입      | 비트 수 (일반)            | 표현 범위 또는 특징                               |
| --- | ------- | -------------------- | ----------------------------------------- |
| 정수  | `int`   | 최소 32비트 (가변 크기)      | 메모리가 허용하는 한 무제한                           |
| 실수  | `float` | 64비트 (고정)            | $-1.7 × 10^{308}$ ~ $1.7 × 10^{308}$ |
| 불리언 | `bool`  | 약 28바이트 (객체 오버헤드 포함) | `True` (1), `False` (0)                   |
| 문자열 | `str`   | 유니코드 기반              | 유니코드 문자 무제한 저장 가능                         |



> 💡 Tip: C/C++ 같은 저수준 언어에서는 메모리 효율과 오버플로우 방지를 위해 데이터 타입의 크기와 표현 범위를 신중하게 고려해야 합니다. 반면 파이썬은 개발 편의성과 임의 정밀도를 제공하지만, 대규모 데이터 처리 시 객체 오버헤드를 이해하고 NumPy 같은 효율적인 라이브러리를 활용하여 메모리를 최적화하는 것이 중요합니다.

## 왜 알아두면 좋을까?
제가 비트, 바이트, 그리고 데이터 표현 방식에 관심을 갖게 된 계기는 AI 모델을 만들며 파이썬을 사용하던 중 다음과 같은 의문이 생겼기 때문입니다:

"파이썬은 고수준 객체지향 언어라 메모리 제어도 어렵고 연산 속도도 느린데, 왜 대부분의 AI는 파이썬으로 개발될까?"

LLM에게 이 질문을 던졌을 때 그 답변의 핵심은 다음과 같았습니다:

"AI 프레임워크는 실제로 C/C++로 구축되어 있으며, 파이썬은 그것을 제어하는 고수준의 인터페이스 역할을 한다."

우리가 파이썬 코드 한 줄로 호출하는 행렬 연산이나 미분 계산은, 내부적으로 수천 줄의 최적화된 C/C++ 코드가 실행되고 있는 것입니다. 예를 들어, LLM의 방대한 가중치를 GPU 메모리에 효율적으로 올리기 위한 양자화 기법이나, 신경망의 복잡한 연산을 위한 병렬 처리는 이러한 하드웨어와 저수준 언어의 이해 없이는 불가능합니다. 파이썬이 제공하는 생산성 뒤에는 C/C++이 제공하는 압도적인 성능이 숨어있으며, 이 둘의 조화가 바로 파이썬이 AI 생태계에서 압도적인 위치를 차지하는 이유입니다.

결국, 데이터 타입과 하드웨어 동작 방식에 대한 깊은 이해는 단순히 언어의 문법을 아는 것을 넘어, 코드가 컴퓨터에서 실제로 어떻게 작동하고 어떤 자원을 사용하는지 파악하는 능력을 길러줍니다. 이는 비단 AI 분야뿐만 아니라, 모든 소프트웨어 개발에서 더 효율적이고 안정적인 코드를 작성하고 시스템의 한계를 극복하는 데 필수적인 역량이 됩니다.
