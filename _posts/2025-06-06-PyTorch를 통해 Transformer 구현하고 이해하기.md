---
layout: single
title: "PyTorchë¥¼ í†µí•´ Transformer êµ¬í˜„í•˜ê³  ì´í•´í•˜ê¸°"
excerpt: "Transformer ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³  PyTorchë¡œ ì§ì ‘ êµ¬í˜„í•´ë³´ëŠ” ê³¼ì •ì„ í†µí•´ AI ëª¨ë¸ì˜ í•µì‹¬ ê°œë…ì„ í•™ìŠµí•©ë‹ˆë‹¤."
categories:
  - AI
  - Deep Learning
tags:
  - PyTorch
  - Transformer
  - Attention
  - NLP
  - Machine Learning
date: 2025-06-06
last_modified_at: 2025-06-06
toc: true
toc_sticky: true
author_profile: true
---

#  1. ğŸ” ì™œ Transformerì¸ê°€?


## RNN, LSTM ë“± ê¸°ì¡´ ëª¨ë¸ì˜ í•œê³„

- **ë³‘ë ¬ ì²˜ë¦¬ ë¶ˆê°€ : ìˆœì°¨ì ìœ¼ë¡œ ë“¤ì–´ì™€ ê³„ì‚°ì„ í•˜ê¸° ë•Œë¬¸ì— ì´ì „ íƒ€ì„ì˜ ë°ì´í„°ê°€ ì¶œë ¥ë˜ê¸° ì „ì—” ê³„ì‚° ë¶ˆê°€ëŠ¥**
- **ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ : ë§¤ìš° í° í…ìŠ¤íŠ¸, ì‹œê³„ì—´ ì •ë³´ê°€ ë“¤ì–´ì˜¤ë©´ ì•ìª½ì˜ ì…ë ¥ ì •ë³´ê°€ ë’¤ë¡œ ì „ë‹¬ì´ ì˜ ë˜ì§€ ì•ŠìŒ (ê·¸ë˜ë””ì–¸íŠ¸ ì†ì‹¤)**
- **ì—°ì‚°ì´ ëŠë¦¼ : ê²Œì´íŠ¸ êµ¬ì¡°ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— í•™ìŠµ ì†ë„ë„ ëŠë¦¼**

## Attentionì˜ ë“±ì¥

**Attention ë©”ì»¤ë‹ˆì¦˜ì€ ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ìœ¼ë¡œ ë“±ì¥í–ˆìŠµë‹ˆë‹¤.
íŠ¹ì • ì…ë ¥ ë‹¨ì–´ê°€ ì¶œë ¥ ë‹¨ì–´ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ì£¼ëŠ”ì§€ ê°€ì¤‘ì¹˜ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. "ì¤‘ìš”í•œ ì •ë³´ì— ë” ì§‘ì¤‘í•˜ì." ì´ê²Œ ë°”ë¡œ "Attention Score"ì…ë‹ˆë‹¤.**

## Transformerì˜ í˜ì‹ 

**2017ë…„ êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ "Attention is All You Need" ë…¼ë¬¸ì€ RNN, LSTM ì—†ì´ ì˜¤ì§ Attentionë§Œìœ¼ë¡œ ìì—°ì–´ ì²˜ë¦¬ ì„±ëŠ¥ì„ í¬ê²Œ ëŒì–´ ì˜¬ë ¸ìŠµë‹ˆë‹¤.**

- **ì™„ì „ ë³‘ë ¬í™” ê°€ëŠ¥ (GPU í•™ìŠµ ê°€ì†)**

- **í° í…ìŠ¤íŠ¸ë„ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬**

- **ì´í›„ ë“±ì¥í•œ BERT, GPT ê°™ì€ ëª¨ë¸ì˜ ê¸°ë°˜**

# âš™ï¸ 2. Transformer êµ¬ì¡°

![](https://velog.velcdn.com/images/hana_dorobo/post/657628cc-86f4-42d8-84b4-85b4edca0498/image.JPG)

**ìœ„ ì´ë¯¸ì§€ê°€ Attention is All You Needì— ì‚½ì…ëœ ì´ë¯¸ì§€ì…ë‹ˆë‹¤.
ì´ ì¤‘ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ë°•ìŠ¤ ì³ì§„ ë¶€ë¶„ ì¤‘ ì™¼ìª½ì´ Encoder Layer ì˜¤ë¥¸ìª½ì´ Decoder Layerì…ë‹ˆë‹¤. ë˜í•œ ì¸ì½”ë”ëŠ” Nê°œ(ëˆˆë¬¸ ê¸°ì¤€ 6)ì˜ ì¸ì½”ë” ë ˆì´ì–´ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©° ë””ì½”ë” ë˜í•œ Nê°œì˜ ë””ì½”ë” ë ˆì´ì–´ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.**
## íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì„ ë“¤ì–´ê°€ê¸° ì „ Positional Encodingì´ë€?

**TransformerëŠ” ìœ„ì—ì„œ ì„¤ëª… ë“œë¦° ê²ƒ ì²˜ëŸ¼ ë¬¸ì¥, ì‹œê³„ì—´ ë°ì´í„°ê°€ ìˆœì°¨ì ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ê²ƒì´ ì•„ë‹Œ ëª¨ë“  ë°ì´í„°ê°€ í•œë²ˆì— ë“¤ì–´ê°€ëŠ” í˜•íƒœì…ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ TransformerëŠ” ìœ„ì¹˜ ì •ë³´ë¥¼ ì•Œì§€ ëª»í•œì±„ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì…ë ¥ëœ ë²¡í„°ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€í•´ì£¼ëŠ” ê²ƒì´ Postional Encodingì…ë‹ˆë‹¤.**

![](https://velog.velcdn.com/images/hana_dorobo/post/ee0fa88a-9988-4886-8080-12c008d69c95/image.png)


**ë…¼ë¬¸ Attention is All You Needì—ì„œ ì‚¬ìš©ëœ ë°©ì‹ì€ Sinusoidal Positional Encodingìœ¼ë¡œ sinê³¼ cosí•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì§ìˆ˜ ì°¨ì› í™€ìˆ˜ ì°¨ì›ì„ ì§€ì •í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ì¸ì ‘í•œ ìœ„ì¹˜ë¼ë¦¬ëŠ” ìœ ì‚¬í•œ ê°’ì„ ê°–ë„ë¡ ë©€ì–´ì§„ ìœ„ì¹˜ëŠ” ì ì°¨ ë‹¤ë¥¸ ê°’ì„ ê°€ì§€ë„ë¡ ì„¤ê³„í•œ ê²ƒì…ë‹ˆë‹¤.**

| position | dim 0    | dim 1    | dim 2   | dim 3   | dim 4  | dim 5 | dim 6        | dim 7 |
| -------- | -------- | -------- | ------- | ------- | ------ | ----- | ------------ | ----- |
| pos 0    | 0.00000  | 1.00000  | 0.00000 | 1.00000 | 0.0000 | 1.0   | 0.000000e+00 | 1.0   |
| pos 1    | 0.84147  | 0.54030  | 0.01000 | 0.99995 | 0.0001 | 1.0   | 1.000000e-06 | 1.0   |
| pos 2    | 0.90930  | -0.41615 | 0.02000 | 0.99980 | 0.0002 | 1.0   | 2.000000e-06 | 1.0   |
| pos 3    | 0.14112  | -0.98999 | 0.03000 | 0.99955 | 0.0003 | 1.0   | 3.000000e-06 | 1.0   |
| pos 4    | -0.75680 | -0.65364 | 0.03999 | 0.99920 | 0.0004 | 1.0   | 4.000000e-06 | 1.0   |

 (d_modelì´ 8ì¸ ì˜ˆì‹œ í‘œ)
 
 **Transformer ëª¨ë¸ì´ ë°œì „ í™•ì¥ë˜ë©´ì„œ Postional Encodingì˜ ë°©ë²•ë„ ì—¬ëŸ¬ê°œë¡œ ë‚˜ë‰©ë‹ˆë‹¤.
 **
 - Learnable Positional Embedding (í•™ìŠµí˜•)
 - Relative Positional Encoding (ìƒëŒ€ ìœ„ì¹˜)
 - Rotary Positional Embedding (RoPE)
 
 **ìœ„ì— ë‚˜ì—´ëœ Positional Encodingì€ ì´ ê¸€ì—ì„œëŠ” ë‹¤ë£¨ì§€ ì•Šê² ìŠµë‹ˆë‹¤.**
 



## ì¸ì½”ë” ë“¤ì–´ê°€ê¸° ì „ ì•Œì•„ì•¼ í•  Q, K, V ì˜ë¯¸

**- Query (Q): íŠ¹ì • ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ì™€ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºê³  ìˆëŠ”ì§€ "ì§ˆë¬¸"í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì–´ë–¤ ì •ë³´ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì¼ì§€ íŒë‹¨í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.**

**- Key (K): ê° ë‹¨ì–´ê°€ ê°€ì§„ ì •ë³´ë¥¼ "ì—´ì‡ "ì²˜ëŸ¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‹¤ë¥¸ Queryì™€ ë¹„êµí•˜ì—¬ ë‹¨ì–´ ê°„ì˜ ì—°ê´€ì„±ì„ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.**

**- Value (V): ê° ë‹¨ì–´ê°€ ê°€ì§„ ì‹¤ì œ ì •ë³´ë¥¼ "ê°’"ìœ¼ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Keyì™€ ë¹„êµí•˜ì—¬ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ , ê·¸ì— ë”°ë¼ ì •ë³´ë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë½‘ì•„ëƒ…ë‹ˆë‹¤. **

## ì¸ì½”ë” ë ˆì´ì–´ì˜ êµ¬ì¡°

- **Multi Head Attention**
- **Feed Forward Network**
- **ì”ì°¨ ì—°ê²° + NormLayer**


### MultiHead Attentionì´ë€?
![](https://velog.velcdn.com/images/hana_dorobo/post/e92ea30b-2bc8-4282-90a6-af379e6b5e65/image.png)
**MultiHead Attentionì€ ìš°ì„  Q, K, Vì˜ ë²¡í„° ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ 3ê°œì˜ Linear(FC, Dense)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. Q,K,Vì˜ Attention Scoreì˜ ê°’ì„ ì–»ì€ ë’¤ Scaled Dot-Product Attentionìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.**

**ì—¬ê¸°ì„œ Scaled Dot-Product Attentionì´ë€ Q, Kì˜ ê°€ì¤‘í•©ì„ êµ¬í•œ ë’¤ Softmax í•¨ìˆ˜ë¡œ í™•ë¥ í™”ë¥¼ ì‹¤ì‹œí•œ ë’¤ ë‹¤ì‹œ Vì˜ ê°€ì¤‘í•©ì„ êµ¬í•©ë‹ˆë‹¤. ì´ê²ƒì´ í•˜ë‚˜ì˜ Scaled Dot-Product Attentionì…ë‹ˆë‹¤.**

**ì´ë ‡ê²Œ Scaled Dot-Product Attentionì—ì„œ ë‚˜ì˜¨ ê°€ì¤‘í•©ì„ Concat ì¦‰ ë³‘í•©í•˜ì—¬ Linear ë ˆì´ì–´ë¡œ í™•ë¥ ê°’ì„ ë‚´ëŠ” ê²ƒì´ í•˜ë‚˜ì˜ MultiHead Attentionì…ë‹ˆë‹¤.**

### Feed Forward NetworkëŠ” ë¬´ìŠ¨ ì—­í• ?
**ë…¼ë¬¸ ê¸°ì¤€ìœ¼ë¡œ êµ¬í˜„ ëœ FFNì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.**
```
nn.Sequential(
    nn.Linear(d_model, d_ff),
    nn.ReLU(),
    nn.Linear(d_ff, d_model)
)
```
**ì‘ì„±ëœ ì½”ë“œë¥¼ ë³´ë©´ 2ê°œì˜ FCë ˆì´ì–´ì™€ ReLUë ˆì´ì–´ê°€ ìˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ FCë ˆì´ì–´ëŠ” ë“¤ì–´ê°€ëŠ” ì°¨ì› d_model=512ì¼ ê²½ìš° d_ff = 2048ë¡œ ì°¨ì›ì„ í™•ì¥í•©ë‹ˆë‹¤. ì´í›„ ReLU í™œì„±í™” ë ˆì´ì–´ë¥¼ í†µí•´ ëª¨ë¸ì— ë¹„ì„ í˜•ì„±ì„ ë„ì…í•˜ì—¬ ë³µì¡í•œ íŒ¨í„´ë„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë‘ ë²ˆì§¸ FCë ˆì´ì–´ë¥¼ í†µí•´ í™•ì¥ëœ ì°¨ì›ì„ ë‹¤ì‹œ ì›ë˜ëŒ€ë¡œ ëŒë ¤ë†“ëŠ” ê²ƒì´ FNNì…ë‹ˆë‹¤. Transformerì—ì„œì˜ FFNì˜ ì—­í• ì€ Attentionì„ í†µí•´ ê° ë‹¨ì–´ì˜ ê´€ê³„ì„± ìœ ì‚¬ë„ë¥¼ ì‚°ì¶œí•œ í›„ ë‹¤ì‹œ í•œë²ˆ ì •ë¦¬ë¥¼ í•˜ëŠ” ì—­í• ì„ FFNì´ í•©ë‹ˆë‹¤.**

### ì”ì°¨ ì—°ê²° + ì •ê·œí™”

**ì”ì°¨ ì—°ê²°ì´ë€ íŠ¹ì • ë¸”ë¡ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ë”í•´ ì •ë³´ ì†ì‹¤ì„ ë°©ì§€í•˜ê³ , ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ì„ ì›í™œí•˜ê²Œ í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•œ ì½”ë“œì…ë‹ˆë‹¤. (ResNetì—ì„œ ì²˜ìŒ ë“±ì¥í•œ ì•„ì´ë””ì–´)**
```
residual = x
x = some_layer(x)
x = x + residual  # ì”ì°¨ ì—°ê²°

```
**ì •ê·œí™”ëŠ” ê° ì‹œì ë³„ë¡œ ë²¡í„°ì˜ í†µê³„ë¥¼ ì •ê·œí™” í•©ë‹ˆë‹¤. ì •ê·œí™”ëŠ” í•™ìŠµì„ ì•ˆì •í™” í•˜ë©° í­ì£¼í•˜ì—¬ ì‚¬ë¼ì§€ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë§‰ìŠµë‹ˆë‹¤.**

<span style="font-size:30px">**ìœ„ 3ê°€ì§€ì˜ ê°œë…ì„ í•˜ë‚˜ë¡œ í•©ì³ ë†“ì€ ê²ƒì´ ì¸ì½”ë” ë ˆì´ì–´ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ ì¸ì½”ë” ë ˆì´ì–´ë¥¼ Nê°œë¡œ ìŒ“ê²Œ ëœë‹¤ë©´ í•˜ë‚˜ì˜ ì¸ì½”ë”ê°€ ì™„ì„± ë©ë‹ˆë‹¤.**</span>

## ë””ì½”ë” ë ˆì´ì–´ì— ì¶”ê°€ ëœ êµ¬ì¡°

- **Masked MulitHead Attention**
- **Encoder - Decoder Attention (Cross Attention)**

### Masked MultiHead Attentionì´ë€?
![](https://velog.velcdn.com/images/hana_dorobo/post/faef1b38-3b7f-4dae-bdc6-92c85aa3fc00/image.jpg)
**ìœ„ ì´ë¯¸ì§€ëŠ” ìœ„ì˜ ì¸ì½”ë” ë ˆì´ì–´ì— ìˆëŠ” ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì„ ì„¤ëª…í•  ë•Œ ì‚¬ìš©í•œ ì´ë¯¸ì§€ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ Scaled Dot-Product Attentionì— ì³ì§„ ë¹¨ê°„ ë°•ìŠ¤ë¥¼ ë³´ë©´ Mask(opt.)ì´ ìˆìŠµë‹ˆë‹¤. ì¸ì½”ë” ë ˆì´ì–´ì—ì„œëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì˜ íŒ¨ë”© í† í°ì„ ë¬´ì‹œí•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ íŒ¨ë”© ë§ˆìŠ¤í¬(Padding Mask)ë§Œ ì‚¬ìš©ì´ ë˜ì§€ë§Œ ë””ì½”ë”ì—ëŠ” ë‹¤ë¥¸ ë§ˆìŠ¤í¬ ê¸°ë²•ì´ ì‚¬ìš© ë©ë‹ˆë‹¤.**

### ë””ì½”ë”ì—ëŠ” ì–´ë–¤ Maskê°€ ì‚¬ìš©ë˜ë‚˜?
**ë””ì½”ë”ì— ë‹¤ë¥¸ Mask ê¸°ë²• í•„ìš”í•œ ì´ìœ ëŠ” Transformerê°€ ë‚˜ì˜¨ ì´ìœ ë¥¼ ë³´ë©´ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. TransformerëŠ” êµ¬ê¸€ì—ì„œ ê¸°ê³„ ë²ˆì—­ì„ ìœ„í•´ ë‚˜ì˜¨ ëª¨ë¸ì…ë‹ˆë‹¤.**
![](https://velog.velcdn.com/images/hana_dorobo/post/d119f2ac-b04d-4277-a7e4-39edfa6a287c/image.jpg)

(ì˜ˆì‹œì˜ í¸ì˜ë¥¼ ìœ„í•´ ì˜ì–´ì™€ í•œêµ­ì–´ì˜ ì–´ìˆœì„ ê°™ê²Œ í•¨)


**ì¸ì½”ë”ì— ë“¤ì–´ê°€ëŠ” "I like money" ë¼ëŠ” ë¬¸ì¥ì„ ë²ˆì—­ í•˜ê¸° ìœ„í•´ì„  ë””ì½”ë”ì—ë„ "ë‚˜ëŠ” ì¢‹ì•„ ëˆì´"ë¼ëŠ” ë¬¸ì¥ë„ ë””ì½”ë”ì— ë„£ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ ë””ì½”ë”ëŠ” ë¬¸ì¥ì„ ì™¼ìª½ë¶€í„° ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìƒì„±í•´ì•¼ë§Œ í•˜ëŠ”ë° ì…ë ¥ì´ ë“¤ì–´ê°ˆ ë•Œ í•œ ë²ˆì— ë‹¤ ë“¤ì–´ê°€ëŠ” êµ¬ì¡°ë¼ ë¯¸ë˜ ì •ë³´ ì¦‰ "ëˆì´"ì— ëŒ€í•œ ë‹µì„ ì´ë¯¸ ì•Œê³  ìˆëŠ” ìƒíƒœì…ë‹ˆë‹¤.**

**ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ê¸°ë²•ìœ¼ë¡œ ìºì£¼ì–¼ ë§ˆìŠ¤í¬(Causal Mask)ê°€ ìˆìŠµë‹ˆë‹¤.ì•Œê³  ìˆëŠ” ì •ë‹µì„ ê°€ë ¤ì„œ ì •ë‹µì„ ì•Œì§€ ëª»í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "ë‚˜ëŠ” ì¢‹ì•„ ëˆì´" ì—ì„œ "ëˆì´"ë¥¼ ë§ˆìŠ¤í¬ë¡œ ê°€ë ¤ì„œ "ë‚˜ëŠ” ì¢‹ì•„"ë¼ëŠ” ë¬¸ì¥ì´ ëœë‹¤ë©´ ë””ì½”ë”ì—ì„œëŠ” ì •ë‹µì´ ê°€ë ¤ì ¸ ìˆê¸° ë•Œë¬¸ì— ì¸ì½”ë”ì—ì„œ ìœ ì‚¬í•œ ê°’ì„ ì°¾ì•„ì„œ ë‹¨ì–´ë¥¼ ìƒì„± í•´ì•¼ í•©ë‹ˆë‹¤.**

### Encoder - Decoder Attention (Cross Attention)
**ì´ì œ ë””ì½”ë”ì—ì„œ ë§ˆìŠ¤í¬ ëœ ë¬¸ì¥, ë‹¨ì–´ë¥¼ ìƒì„± í•˜ê¸° ìœ„í•´ì„œëŠ” ë””ì½”ë”ì— ìˆëŠ” ì •ë³´ë§Œìœ¼ë¡  ë¶€ì¡±í•©ë‹ˆë‹¤. ê·¸ë ‡ê²Œ ë˜ë©´ ì¸ì½”ë”ì—ì„œ ë§Œë“  "I like money"ì˜ ì •ë³´ë¥¼ ì°¸ì¡°í•´ì•¼ í•©ë‹ˆë‹¤.
ì´ ë•Œ í•„ìš”í•˜ê²Œ ë˜ëŠ” ê²ƒì´ Encoder - Decoder Attention (Cross Attention)ì…ë‹ˆë‹¤.**

## Transformer ëª¨ë¸

![](https://velog.velcdn.com/images/hana_dorobo/post/0a255b6d-5bd4-4fd7-8783-4cd2bc3c9ee1/image.png)

<span style="font-size:28px">** ì´ì œ ìœ„ì—ì„œ ì„¤ëª…í•œ PEë¥¼ ì ìš©í›„ MHAì™€ FFNìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì¸ì½”ë” ë ˆì´ì–´ê°€ ì™„ì„±ë˜ê³  ì¶”ê°€ë¡œ Masked, Cross ì–´í…ì…˜ì„ ì ìš©í•˜ë©´ ë””ì½”ë” ë ˆì´ì–´ê°€ ë©ë‹ˆë‹¤. ì´ ì¸ì½”ë”, ë””ì½”ë” ë ˆì´ì–´ë¥¼ Nê°œ ìŒ“ì€ í›„ Linear ë ˆì´ì–´ì™€ Softmaxë¥¼ í†µê³¼í•˜ë©´ í•˜ë‚˜ì˜ Transformer ëª¨ë¸ì´ ì™„ì„± ë©ë‹ˆë‹¤. **</span>

# 3. ğŸ› ï¸ PyTorchë¡œ Transformer ì§ì ‘ êµ¬í˜„í•˜ê¸°
- ## PositionalEncoding
```
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()

        # (max_len, d_model) í¬ê¸°ì˜ ìœ„ì¹˜ í–‰ë ¬ ìƒì„±
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)  # ì§ìˆ˜: sin
        pe[:, 1::2] = torch.cos(position * div_term)  # í™€ìˆ˜: cos

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)  # í•™ìŠµë˜ì§€ ì•ŠëŠ” ë²„í¼ë¡œ ì €ì¥

    def forward(self, x):
        """
        x: (batch_size, seq_len, d_model)
        return: x + positional_encoding
        """
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]
```
**PositionalEncodingì€ torchì˜ ë‚´ì¥ `sin`, `cos` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.  
ì§ìˆ˜ ì°¨ì›ì—ëŠ” `sin`, í™€ìˆ˜ ì°¨ì›ì—ëŠ” `cos` ê°’ì„ ì ìš©í•´ ì„œë¡œ ë‹¤ë¥¸ ì£¼ê¸°ì˜ í•¨ìˆ˜ë¡œ ìœ„ì¹˜ë¥¼ í‘œí˜„í•¨ìœ¼ë¡œì¨  
ìœ„ì¹˜ë§ˆë‹¤ ê³ ìœ í•œ íŒ¨í„´ì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤. ì´ ë°©ì‹ì€ í•™ìŠµ ì—†ì´ë„ ìœ„ì¹˜ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê³ ,  
í† í° ê°„ ìƒëŒ€ì  ê±°ë¦¬ ì •ë³´ë¥¼ ë³´ì¡´í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.**

**ë˜í•œ, `register_buffer`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ëœ í¬ì§€ì…”ë„ ì¸ì½”ë”©ì„ ë“±ë¡í•˜ëŠ”ë°,  
ì´ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ì²˜ëŸ¼ GPUë¡œ ì´ë™ë˜ê±°ë‚˜ `.state_dict()`ì— ì €ì¥ë˜ì§€ë§Œ, optimizerë¡œ í•™ìŠµë˜ì§€ ì•ŠëŠ” ê³ ì •ëœ ê°’ì…ë‹ˆë‹¤.  
ì¦‰, í•™ìŠµ ëŒ€ìƒì´ ì•„ë‹Œ ìƒíƒœ ì •ë³´ë¥¼ ëª¨ë¸ì— í¬í•¨ì‹œí‚¬ ë•Œ ìœ ìš©í•œ PyTorchì˜ ê¸°ëŠ¥ì…ë‹ˆë‹¤.**

- ## ScaledDotProductAttention
```
class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super().__init__()


    def forward(self, Q, K, V, mask=None):
        """
        Q: (batch_size, heads, seq_len_q, d_k)
        K: (batch_size, heads, seq_len_k, d_k)
        V: (batch_size, heads, seq_len_k, d_v)
        mask: (batch_size, heads, seq_len_q, seq_len_k) or None
            - ì¸ì½”ë” self-attention: src_mask (íŒ¨ë”© ë§ˆìŠ¤í¬)
            - ë””ì½”ë” self-attention: tgt_mask (ìºì£¼ì–¼ ë§ˆìŠ¤í¬)
            - ë””ì½”ë” cross-attention: memory_mask (íŒ¨ë”© ë§ˆìŠ¤í¬)
        """
        # 1. Attention score ê³„ì‚°: Q @ K^T / sqrt(d_k)
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(d_k)

        # 2. maskê°€ ìˆë‹¤ë©´ -infì²˜ë¦¬
        # maskê°€ 0ì¸ ìœ„ì¹˜(íŒ¨ë”© ë˜ëŠ” ë¯¸ë˜ í† í°)ì— -infë¥¼ í• ë‹¹í•˜ì—¬ softmax í›„ 0ì´ ë˜ë„ë¡ í•¨
        if mask is not None:
            scores = scores.masked_fill(mask==0, float('-inf'))
        
        # 3. Softmax í™•ë¥ í™”
        atten_weight = F.softmax(scores, dim=-1)
        
        # 4. Attention ì ìš©: Vì— ê°€ì¤‘í•©
        output = torch.matmul(atten_weight, V)

        return output, atten_weight
```
**ì œê°€ êµ¬í˜„í•œ ScaledDotProductAttention ì½”ë“œì…ë‹ˆë‹¤. ìœ„ì—ì„œ ì„¤ëª…í•œ ê²ƒ ì²˜ëŸ¼ ê° Q, Kì˜ ê°’ì„ ì…ë ¥ ë°›ì•„ ê°€ì¤‘í•©ìœ¼ë¡œ êµ¬í•œ ë’¤ Softmaxë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ì‹œ Vì™€ ê°€ì¤‘í•©ì„ êµ¬í•˜ì—¬ ëŠ” ì½”ë“œ ì…ë‹ˆë‹¤. ë˜í•œ Maskë¡œ src_maskë¥¼ ë°›ëŠ”ë‹¤ë©´ ì¸ì½”ë”ì— ë“¤ì–´ê°€ëŠ” ë§ˆìŠ¤í¬ ê¸°ë²•(íŒ¨ë”©) tgt_maskë¥¼ ë°›ê²Œ ëœë‹¤ë©´ ë””ì½”ë” ë§ˆìŠ¤í¬ ê¸°ë²•(ìºì£¼ì–¼)ì„ ì‚¬ìš©í•˜ì—¬ masked_fill í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•„ìš”í•œ ì°¨ì›ì„ -infë¡œ ê°€ë ¤ ë²„ë¦½ë‹ˆë‹¤.**

- ## Multi-Head Attention

```
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "d_modelì€ num_headsë¡œ ë‚˜ëˆ„ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)

        self.attention = ScaledDotProductAttention()

    def forward(self, x, mask=None, k_v=None):
        """
        x: query (B, seq_len_q, d_model)
        k_v: (B, seq_len_kv, d_model), Noneì´ë©´ xë¥¼ ì‚¬ìš© (self-attention)
        mask: (B, 1, seq_len_q, seq_len_kv) or None
        """
        if k_v is None:
            K, V = x, x
        else:
            K, V = k_v, k_v

        B, L_q, _ = x.size()
        L_k = K.size(1)

        Q = self.q_linear(x)
        K = self.k_linear(K)
        V = self.v_linear(V)

        # shape ë³€ê²½: (B, num_heads, seq_len, d_k)
        Q = Q.view(B, L_q, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(B, L_k, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(B, L_k, self.num_heads, self.d_k).transpose(1, 2)

        # attention ìˆ˜í–‰
        out, atten_weight = self.attention(Q, K, V, mask=mask)

        # ë³‘í•© í›„ ìµœì¢… projection
        out = out.transpose(1, 2).contiguous().view(B, L_q, self.d_model)
        out = self.out_proj(out)

        return out, atten_weight
```
**Multi-Head Attentionì€ ë“¤ì–´ì˜¨ ë¬¸ì¥, ì‹œê³„ì—´ì„ 3ê°œì˜ FCë ˆì´ì–´ë¥¼ í†µí•´ Q, K, Vì˜ ì„ í˜• ë³€í™˜ìœ¼ë¡œ ìƒì„±ì„ í•©ë‹ˆë‹¤. ì—¬ëŸ¬ê°œì˜ headë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì„ ì—¬ëŸ¬ê°œì˜ Attentionì´ ë³‘ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.**
```
self.out_proj = nn.Linear(d_model, d_model)

```
**ì´í›„ ì—¬ëŸ¬ê°œì˜ headì˜ Attention ê²°ê³¼ë¥¼ concatí•œ í›„ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ë‹¤ì‹œ projection í•©ë‹ˆë‹¤.**

```
def forward(self, x, mask=None, k_v=None):

```
**forward ë©”ì„œë“œì—ì„œ xë¼ëŠ” ì¿¼ë¦¬ë¥¼ ì…ë ¥ ë°›ê³  Key/Valueë¥¼ ìœ„í•œ k_vë¼ëŠ” ì¸ìë¥¼ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ë˜í•œ maskê°€ í•„ìš”í•˜ë‹¤ë©´ ê°’ì„ í• ë‹¹í•˜ë©´ ë©ë‹ˆë‹¤.**

```
if k_v is None:
    K, V = x, x
else:
    K, V = k_v, k_v

```
**k_vê°€ Noneì´ë©´ Self-Attention ì§„í–‰ k_vê°€ ì£¼ì–´ì§€ë©´ Cross-Attentionì„ ì§„í–‰í•©ë‹ˆë‹¤.**

```
mha = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)
query = key = value = torch.randn(32, 10, 512)
out, attn_weights = mha(query, key, value)
```
_(Torchì˜ Multi-Head Attention ì‚¬ìš©ë²•)_



- ## FeedForwardNetwork
```
class FeedForwardNetwork(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()

        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)

    def forward(self , x):
        # x : (batch_size, seq_len, d_model)
        return self.linear2(self.dropout(torch.relu(self.linear1(x))))
```
**FFNì€ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì„¤ëª…ì„ ìƒëµí•˜ê² ìŠµë‹ˆë‹¤.**

- ## EncoderLayer
```
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()

        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # Multi-Head Attention + Add & Norm
        atten_output, _ = self.mha(x, mask=mask) # mask ì¸ì ëª…ì‹œ
        x = self.norm1(x + self.dropout(atten_output)) # ì”ì°¨ ì—°ê²° + ì •ê·œí™”

        # FeedForwad + Add & Norm
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))

        return x
```
**ì¸ì½”ë” ë ˆì´ì–´ëŠ” ìœ„ì—ì„œ êµ¬í˜„í•œ MultiHeadAttention(MaskëŠ” Padding Mask ì‚¬ìš©)ê³¼ FeedForwardNetworkë¥¼ í†µí•´ ê°’ì„ êµ¬í•©ë‹ˆë‹¤.**

```
ffn_output = self.ffn(x)
x = self.norm2(x + self.dropout(ffn_output))

```
**ë§ˆì§€ë§‰ìœ¼ë¡œ LayerNormì„ í†µí•´ ê° ì”ì°¨ ì—°ê²° í›„ì— ì •ê·œí™” ì ìš© Dropoutì„ ì‚¬ìš©í•˜ì—¬ ê³¼ì í•© ë°©ì§€ë¥¼ í•©ë‹ˆë‹¤.**


- ## Encoder
```
class Encoder(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1): # max_len ì œê±°
        super().__init__()

        # í¬ì§€ì…”ë„ ì¸ì½”ë”©ì€ Encoderë¥¼ ì‚¬ìš©í•˜ëŠ” ì™¸ë¶€ ëª¨ë“ˆ(Transformer)ì—ì„œ ì²˜ë¦¬
        # self.pos_encoder = PositionalEncoding(d_model, max_len) 

        # ì¸ì½”ë” ë ˆì´ì–´ ìŒ“ê¸°
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask = None):
        # x : (batch_size, seq_len, d_model)
        # PositionalEncodingì€ ì™¸ë¶€ì—ì„œ ì ìš©ëœ í›„ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¨ë‹¤ê³  ê°€ì •
        # x = self.pos_encoder(x) 
        # x = self.dropout(x) # Dropoutì€ PositionalEncoding ì´í›„ ë˜ëŠ” Embedding ì´í›„ì— í•œ ë²ˆ ì ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 

        for layer in self.layers:
            x = layer(x, mask=mask)

        return x
```

**ìœ„ì—ì„œ ì •ì˜í•œ EncoderLayerë¥¼ num_layers ë§Œí¼ ìŒ“ìŠµë‹ˆë‹¤.**


```
encoder_layer = nn.TransformerEncoderLayer(
    d_model=512,
    nhead=8,
    dim_feedforward=2048,
    dropout=0.1,
    activation='relu',
    batch_first=True
)

encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)
```

_(Torchë¡œ ì¸ì½”ë” ë ˆì´ì–´ë§Œ ì‚¬ìš©í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©ë²•)_
- ## DecoderLayer
```
class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()

        # 1. Masked Self-Attention
        self.self_atten = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)

        # 2. Encoder-Decoder Attention
        self.cross_atten = MultiHeadAttention(d_model, num_heads)
        self.norm2 = nn.LayerNorm(d_model)

        # 3. FeedForwardNetwork
        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)
        self.norm3 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_output, tgt_mask, memory_mask):
        # x: (batch_size, tgt_seq_len, d_model)
        # enc_output: (batch_size, src_seq_len, d_model)

        # Masked self-attention (decoder input)
        atten1, _ = self.self_atten(x, mask=tgt_mask) # mask ì¸ì ëª…ì‹œ
        x = self.norm1(x + self.dropout(atten1)) # Add & Norm ì¶”ê°€

        # Cross attention (query: decoder, key/value: encoder output)
        # ì´ì „ xë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì . atten1ì„ queryë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
        atten2, _ = self.cross_atten(x, mask=memory_mask, k_v=enc_output)
        x = self.norm2(x + self.dropout(atten2))

        # Feed Forwad
        ffn_out = self.ffn(x)
        x = self.norm3(x + self.dropout(ffn_out))

        return x
```

**ë””ì½”ë” ë ˆì´ì–´ëŠ” ì„¤ëª…ë“œë¦° ê²ƒ ì²˜ëŸ¼ Self-Attention, Cross-Attentionì„ ì‚¬ìš©í•©ë‹ˆë‹¤.**
```
# Masked self-attention (decoder input)
atten1, _ = self.self_atten(x, mask=tgt_mask) 
# Cross attention (query: decoder, key/value: encoder output)
atten2, _ = self.cross_atten(x, mask=memory_mask, k_v=enc_output)
```
**ì´ë ‡ê²Œ maskì˜ ì¸ìë¥¼ memory_maskë¥¼ ì‚¬ìš©í•˜ì—¬ ìºì£¼ì–¼ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ê³  k_vì˜ ì¸ìë¥¼ ëª…ì‹œí•˜ì—¬ ë””ì½”ë” ë ˆì´ì–´ì— ë§ëŠ” ì–´í…ì…˜ êµ¬ì¡°ë¥¼ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤.**


- ## Decoder
```
class Decoder(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1): # max_len ì œê±°
        super().__init__()
        # PositionalEncodingì€ Decoderë¥¼ ì‚¬ìš©í•˜ëŠ” ì™¸ë¶€ ëª¨ë“ˆ(Transformer)ì—ì„œ ì²˜ë¦¬
        # self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_output, tgt_mask, memory_mask):
        # PositionalEncodingì€ ì™¸ë¶€ì—ì„œ ì ìš©ëœ í›„ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¨ë‹¤ê³  ê°€ì •
        # x = self.pos_encoder(x)
        # x = self.dropout(x) # Dropoutì€ PositionalEncoding ì´í›„ ë˜ëŠ” Embedding ì´í›„ì— í•œ ë²ˆ ì ìš©

        for layer in self.layers:
            x = layer(x, enc_output, tgt_mask, memory_mask)

        return x
```

**ë””ì½”ë”ë„ ì¸ì½”ë”ì™€ ë˜‘ê°™ì´ ë””ì½”ë” ë ˆì´ì–´ë¥¼ num_layersë§Œí¼ ìŒ“ìœ¼ë©´ ë©ë‹ˆë‹¤. ì£¼ì„ìœ¼ë¡œ positionalEncodingì´ ìˆì„ ê²½ìš°ì˜ ì½”ë“œë¥¼ ì‘ì„± í–ˆìŠµë‹ˆë‹¤.**

```
decoder_layer = nn.TransformerDecoderLayer(
    d_model=512,
    nhead=8,
    dim_feedforward=2048,
    dropout=0.1,
    activation='relu',
    batch_first=True
)

decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)
```
_(Torchì˜ ë””ì½”ë” ë ˆì´ì–´ë§Œ ì‚¬ìš©í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©ë²•)_

## Transformer

```
class Transformer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, num_layers,
                 src_vocab_size, tgt_vocab_size, max_len=5000, dropout=0.1):
        super().__init__()

        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        self.pos_encoder = PositionalEncoding(d_model, max_len) # PositionalEncoding ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
        self.dropout = nn.Dropout(dropout) # Dropout ì¶”ê°€

        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout) # max_len ì œê±°
        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout) # max_len ì œê±°

        self.output_linear = nn.Linear(d_model, tgt_vocab_size)

    def forward(self, src, tgt, src_mask = None, tgt_mask = None, memory_mask = None):
        """
        src: (B, src_seq_len)  â†’ ì¸ì½”ë” ì…ë ¥
        tgt: (B, tgt_seq_len)  â†’ ë””ì½”ë” ì…ë ¥
        """

        # 1. ì„ë² ë”© ë° í¬ì§€ì…”ë„ ì¸ì½”ë”©
        src_embed = self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim) # ìŠ¤ì¼€ì¼ë§ ì¶”ê°€
        src_embed = self.pos_encoder(src_embed)
        src_embed = self.dropout(src_embed) # Dropout ì ìš©

        tgt_embed = self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim) # ìŠ¤ì¼€ì¼ë§ ì¶”ê°€
        tgt_embed = self.pos_encoder(tgt_embed)
        tgt_embed = self.dropout(tgt_embed) # Dropout ì ìš©


        # 2. ì¸ì½”ë”
        enc_output = self.encoder(src_embed, mask=src_mask)

        # 3. ë””ì½”ë”
        # dec_output = self.decoder(tgt, mask=tgt_mask) # ìˆ˜ì • ì „
        dec_output = self.decoder(tgt_embed, enc_output, tgt_mask, memory_mask) # ìˆ˜ì • í›„

        # 4. Output Linear (ì–¸ì–´ ìƒì„±ì´ë¼ë©´ vocab_size)
        output = self.output_linear(dec_output)


        return output  # (B, tgt_seq_len, tgt_vocab_size)
```

**ì´í›„ ëª¨ë“  ëª¨ë“ˆì„ í•˜ë‚˜ë¡œ í•©ì³ì„œ Transformerë¡œ ë§Œë“¤ë©´ ë©ë‹ˆë‹¤. ì„ë² ë”©ì€ torchì— ìˆëŠ” ë‚´ì¥ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ë””ì½”ë” ë¸”ë¡ì„ ë‚˜ì˜¨ í›„ Linear ë ˆì´ì–´ë¥¼ í†µí•´ ë‹¨ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.**
```
transformer = nn.Transformer(
    d_model=512,
    nhead=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    dim_feedforward=2048,
    dropout=0.1,
    activation='relu',
    layer_norm_eps=1e-5,
    batch_first=True  # (B, L, D) í˜•íƒœë¡œ ì…ë ¥ ë°›ì„ ìˆ˜ ìˆê²Œ
)

```
_(Torchì˜ Transformer ëª¨ë¸ ì‚¬ìš©ë²•)_

# 4. ğŸ“Œ Transformerì˜ í™œìš©ê³¼ ë°œì „ íë¦„

- ## BERT (2018, Google)
<span style="font-size:25px">**BERTë€ Transformerì˜ ì¸ì½”ë” ë¶€ë¶„ë§Œ ì‚¬ìš©í•˜ì—¬ ì£¼ë¡œ ë¬¸ì¥ ë¶„ë¥˜, ì§ˆì˜ ì‘ë‹µ, ë¬¸ì¥ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.**</span>

- ## GPT ì‹œë¦¬ì¦ˆ (2018~í˜„ì¬, OpenAI)
<span style="font-size:25px"> **Generative Pretrained Transformerë¡œ ì‚¬ì „ í•™ìŠµ ëœ íŠ¸ëœìŠ¤í¬ë¨¸ë¼ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. Transformerì˜ ë””ì½”ë” êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.**</span>
- ## T5, BART
<span style="font-size:25px"> **T5ëŠ” "Text-to-Text" ëª¨ë¸ë¡œ ëª¨ë“  NLP ì‘ì—…ì„ í…ìŠ¤íŠ¸ ë³€í™˜ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤. BARTëŠ” BERT + GPTë¡œ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ì…ë ¥ ë°›ì•„ ìƒì„±í•˜ëŠ”ë° íŠ¹í™” ëœ ëª¨ë¸ì…ë‹ˆë‹¤. **</span>

- ## ìì—°ì–´ ì´ì™¸ ë¶„ì•¼ì˜ í™œìš©
<span style="font-size:25px"> **ì´ë¯¸ì§€ ì‹œê³„ì—´, ì£¼ì‹ ë°ì´í„°ë¥¼ ë¶„ì„ í•˜ëŠ”ë° í™œìš© ë˜ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ CNN ì—†ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ”ë°ë„ ì“°ê³  ë©€í‹° ëª¨ë‹¬ë¡œ í™•ì¥í•˜ì—¬ í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ì—ë„ í™œìš©ì´ ë©ë‹ˆë‹¤.**</span>

# 5. ë§ˆì§€ë§‰ìœ¼ë¡œ 


<span style="font-size:25px">**íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” ëŒ€í‘œì ì¸ ì•„í‚¤í…ì²˜ ê·¸ë¦¼ë§Œ ë³´ë©´ ë‹¨ìˆœí•´ ë³´ì´ì§€ë§Œ, ì‹¤ì œë¡œ ì´ë¥¼ ì§ì ‘ êµ¬í˜„í•´ë³´ë©´ ìƒë‹¹íˆ ë‹¤ì–‘í•œ ê°œë…ê³¼ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ìŠµë‹ˆë‹¤.**</span>

<span style="font-size:25px">**ì²˜ìŒì—ëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ì´í•´í•˜ê³ ì MultiHeadAttentionì„ ì‚´í´ë³´ë‹¤ê°€, ìì—°ìŠ¤ëŸ½ê²Œ "ì™œ ì—¬ëŸ¬ ê°œì˜ í—¤ë“œë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?"ë¼ëŠ” ì§ˆë¬¸ìœ¼ë¡œ ì´ì–´ì¡Œê³ , MultiHeadAttention ë‚´ë¶€ì— í¬í•¨ëœ ScaledDotProductAttentionì˜ êµ¬ì¡°ì™€ ì›ë¦¬ë¥¼ ì§ì ‘ êµ¬í˜„í•´ë³´ë©° Q, K, Vì˜ ì˜ë¯¸ì™€ ì—­í• ë„ íŒŒì•…í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.**</span>

<span style="font-size:25px">**ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ ì¸ì½”ë”ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ê³  ì˜ë¯¸ ìˆëŠ” í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ë””ì½”ë”ëŠ” ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–´ë–»ê²Œ ë‹¤ìŒ í† í°(ë‹¨ì–´)ì„ ì˜ˆì¸¡í•˜ëŠ”ì§€ ì ì°¨ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.**</span>

<span style="font-size:25px">**ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ êµ¬ì¡°ë¥¼ ëª…í™•íˆ ì´í•´í•˜ê³  ë‚˜ë‹ˆ, íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ë‹¨ìˆœíˆ ìì—°ì–´ ì²˜ë¦¬ì— êµ­í•œëœ ëª¨ë¸ì´ ì•„ë‹ˆë¼ëŠ” ì ë„ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¸ì½”ë”ëŠ” í…ìŠ¤íŠ¸ ë¶„ë¥˜ë‚˜ ê°ì • ë¶„ì„ ê°™ì€ ì…ë ¥ ì •ë³´ë¥¼ ìš”ì•½í•˜ê±°ë‚˜ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì— ì í•©í•˜ê³ , ë””ì½”ë”ëŠ” ì‹œí€€ìŠ¤ ìƒì„±ì´ í•„ìš”í•œ ë¬¸ì¥ ìƒì„±, ìš”ì•½, ë²ˆì—­, ë˜ëŠ” ì‹œê³„ì—´ ì˜ˆì¸¡ ë“±ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**</span>

<span style="font-size:25px">**ì´ëŸ¬í•œ í™•ì¥ì€ ìì—°ì–´ë¿ ì•„ë‹ˆë¼ ì´ë¯¸ì§€ë‚˜ ì‹œê³„ì—´ ë°ì´í„°ì—ë„ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤ê³  ëŠê¼ˆê³ , íŠ¹íˆ ì´ë¯¸ì§€ ì‹œê³„ì—´ ë¶„ë¥˜ì²˜ëŸ¼ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ í†µí•´ ìƒí™©ì„ íŒë‹¨í•˜ê±°ë‚˜ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ë¬¸ì œì—ë„ íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ê°€ ìœ ìš©í•˜ë‹¤ëŠ” ê°€ëŠ¥ì„±ì„ ë°œê²¬í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.**</span>